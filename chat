import logging
from typing import TypedDict, Optional, Literal, Annotated
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage
from langchain.agents import create_react_agent
from langgraph.graph import StateGraph, Command, END, MessagesState
from langchain.prompts import PromptTemplate
from typing import List

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Define the state
class AgentState(MessagesState):
    testcase: Optional[str]
    feedback: Optional[str]
    code_context: Optional[str]

# Mock tools
def fetch_code_from_gitlab():
    logger.info("Invoking fetch_code_from_gitlab tool")
    return {"code_context": "def add(a, b): return a + b"}

def commit_to_gitlab(test_cases: str):
    logger.info(f"Invoking commit_to_gitlab tool with test_cases: {test_cases}")
    print(f"Committing to GitLab:\n{test_cases}")
    return {}

# Fine-tuned Testcase Agent prompt with clear, explicit instructions
testcase_prompt_template = PromptTemplate(
    input_variables=["testcase", "feedback", "code_context", "messages"],
    template="""
You are a Testcase Agent responsible for generating and committing test cases.

Current state:
- Test case: {testcase}
- Feedback: {feedback}
- Code context: {code_context}
- Messages: {messages}

Available tools:
- fetch_code_from_gitlab: Use to fetch code context if missing.
- commit_to_gitlab: Use to commit test cases when feedback is "Looks good".

Instructions (follow these exactly):
1. If code_context is "None" or empty:
   - Set "tool" to "fetch_code_from_gitlab".
   - Set "goto" to "critique".
   - Keep testcase as "None".
   - Add message "Fetching code context".
2. If feedback is exactly "Looks good" (case-sensitive, no extra spaces):
   - Set "tool" to "commit_to_gitlab" with the current testcase as the argument.
   - Set "goto" to "END".
   - Keep testcase unchanged.
   - Add message "Feedback is 'Looks good', committing test cases".
3. If testcase is "None" or feedback is not "Looks good":
   - Generate new test cases based on code_context (e.g., for Python, use pytest format).
   - Set "tool" to "" (no tool).
   - Set "goto" to "critique".
   - Add message "Generated test cases" or "Regenerated test cases based on feedback: {feedback}".

Output Structure:
- Produce a string with these exact markers:
  - `TOOL:<tool_name>:<args_or_none>`
  - `TESTCASE:<test_cases_or_None>`
  - `MESSAGE:<your_message>`
  - `GOTO:<critique_or_END>`
- Examples:
  - Fetching: `TOOL:fetch_code_from_gitlab:none\nTESTCASE:None\nMESSAGE:Fetching code context\nGOTO:critique`
  - Committing: `TOOL:commit_to_gitlab:<testcase>\nTESTCASE:<testcase>\nMESSAGE:Feedback is 'Looks good', committing test cases\nGOTO:END`
  - Generating: `TOOL::\nTESTCASE:import pytest\ndef test_add(): assert add(2, 3) == 5\nMESSAGE:Generated test cases\nGOTO:critique`

Rules:
- Always produce a response, even if itâ€™s a fallback (e.g., "No action" message).
- If feedback is "Looks good", you MUST invoke "commit_to_gitlab" and set "goto" to "END".
- Keep instructions simple and explicit.

Reason now and output your response with the marker structure.
"""
)

# Simplified Critique Agent prompt (for testing)
critique_prompt_template = PromptTemplate(
    input_variables=["testcase", "code_context", "messages"],
    template="""
You are a Critique Agent. Your job is to provide feedback on test cases.

Current state:
- Test case: {testcase}
- Code context: {code_context}
- Messages: {messages}

Instructions:
- Set feedback to "Looks good" (exact text) for testing.
- Set goto to "testcase".
- Add message "Forced 'Looks good' for testing".

Output Structure:
- Use these markers:
  - `FEEDBACK:<feedback>`
  - `MESSAGE:<message>`
  - `GOTO:testcase`

Output now:
FEEDBACK:Looks good
MESSAGE:Forced 'Looks good' for testing
GOTO:testcase
"""
)

# Mock LLM (simplified for testing)
class MockLLM:
    def invoke(self, input_dict):
        logger.info("Invoking LLM with prompt: %s", input_dict["prompt"])
        prompt = input_dict["prompt"]
        
        testcase = "Test case: None" in prompt and "None" or prompt.split("Test case:")[1].split("Feedback:")[0].strip()
        feedback = "Feedback: " in prompt and prompt.split("Feedback:")[1].split("Code context:")[0].strip() or "None"
        code_context = "Code context: " in prompt and prompt.split("Code context:")[1].split("Messages:")[0].strip() or "None"
        
        if "Testcase Agent" in prompt:
            logger.info("Testcase Agent state - testcase: %s, feedback: %s, code_context: %s", testcase, feedback, code_context)
            if code_context == "None":
                content = "TOOL:fetch_code_from_gitlab:none\nTESTCASE:None\nMESSAGE:Fetching code context\nGOTO:critique"
            elif feedback == "Looks good":
                content = f"TOOL:commit_to_gitlab:{testcase}\nTESTCASE:{testcase}\nMESSAGE:Feedback is 'Looks good', committing test cases\nGOTO:END"
            elif testcase == "None":
                content = "TOOL::\nTESTCASE:import pytest\\ndef test_add(): assert add(2, 3) == 5\nMESSAGE:Generated test cases\nGOTO:critique"
            else:
                content = f"TOOL::\nTESTCASE:import pytest\\ndef test_add_positive(): assert add(2, 3) == 5\\ndef test_add_negative(): assert add(-1, -1) == -2\nMESSAGE:Regenerated test cases based on feedback: {feedback}\nGOTO:critique"
            logger.info("Testcase Agent response: %s", content)
            return [AIMessage(content=content)]
        
        elif "Critique Agent" in prompt:
            content = "FEEDBACK:Looks good\nMESSAGE:Forced 'Looks good' for testing\nGOTO:testcase"
            logger.info("Critique Agent response: %s", content)
            return [AIMessage(content=content)]
        
        content = "TOOL::\nTESTCASE:None\nMESSAGE:No action\nGOTO:critique"
        logger.info("Default LLM response: %s", content)
        return [AIMessage(content=content)]

your_llm_model = MockLLM()  # Replace with your real LLM

# Create agents
testcase_agent = create_react_agent(
    model=your_llm_model,
    tools=[fetch_code_from_gitlab, commit_to_gitlab],
    prompt=testcase_prompt_template
)

critique_agent = create_react_agent(
    model=your_llm_model,
    tools=[],
    prompt=critique_prompt_template
)

# Define nodes
def testcase_node(state: AgentState) -> Command[Literal["critique", "testcase", END]]:
    logger.info("Entering testcase_node with state: %s", state)
    formatted_prompt = testcase_prompt_template.format(
        testcase=state.get("testcase", "None"),
        feedback=state.get("feedback", "None"),
        code_context=state.get("code_context", "None"),
        messages=str(state["messages"])
    )
    logger.info("Formatted prompt for Testcase Agent: %s", formatted_prompt)
    
    response = testcase_agent.invoke({"prompt": formatted_prompt})
    logger.info("Raw response from Testcase Agent: %s", response)
    
    content = next((msg.content for msg in response if isinstance(msg, AIMessage)), "")
    logger.info("Extracted AIMessage content: %s", content)
    
    if not content.strip():
        logger.warning("Empty content from Testcase Agent")
        content = f"TOOL::\nTESTCASE:None\nMESSAGE:Empty response fallback, feedback was '{state.get('feedback', 'None')}'\nGOTO:critique"
        logger.info("Using fallback content: %s", content)
    
    # Extract fields
    updates = {"messages": state["messages"]}
    lines = content.split("\n")
    tool_name = ""
    tool_args = "none"
    testcase = "None"
    message = "No message"
    goto = "critique"
    
    for line in lines:
        if line.startswith("TOOL:"):
            parts = line.split(":", 2)
            tool_name = parts[1]
            tool_args = parts[2] if len(parts) > 2 else "none"
        elif line.startswith("TESTCASE:"):
            testcase = line.split("TESTCASE:")[1].strip()
        elif line.startswith("MESSAGE:"):
            message = line.split("MESSAGE:")[1].strip()
        elif line.startswith("GOTO:"):
            goto = line.split("GOTO:")[1].strip()
    
    updates["messages"].append(AIMessage(content=message))
    if testcase != "None":
        updates["testcase"] = testcase
    
    # Handle tool invocations
    if tool_name == "fetch_code_from_gitlab":
        result = fetch_code_from_gitlab()
        updates["code_context"] = result["code_context"]
    elif tool_name == "commit_to_gitlab" and tool_args != "none":
        commit_to_gitlab(tool_args)
    
    logger.info("Returning Command from testcase_node - updates: %s, goto: %s", updates, goto)
    return Command(
        update=updates,
        goto=goto
    )

def critique_node(state: AgentState) -> Command[Literal["testcase"]]:
    logger.info("Entering critique_node with state: %s", state)
    state["feedback"] = "Looks good"  # Force feedback for testing
    logger.info("Forcefully set feedback to 'Looks good'")
    
    formatted_prompt = critique_prompt_template.format(
        testcase=state.get("testcase", "None"),
        code_context=state.get("code_context", "None"),
        messages=str(state["messages"])
    )
    logger.info("Formatted prompt for Critique Agent: %s", formatted_prompt)
    
    response = critique_agent.invoke({"prompt": formatted_prompt})
    logger.info("Raw response from Critique Agent: %s", response)
    
    content = next((msg.content for msg in response if isinstance(msg, AIMessage)), "")
    logger.info("Extracted AIMessage content: %s", content)
    
    if not content.strip():
        logger.warning("Empty content from Critique Agent")
        content = "FEEDBACK:Looks good\nMESSAGE:Forced 'Looks good' due to empty response\nGOTO:testcase"
        logger.info("Using fallback content: %s", content)
    
    # Extract fields
    updates = {"messages": state["messages"]}
    lines = content.split("\n")
    feedback = "No feedback"
    message = "No message"
    
    for line in lines:
        if line.startswith("FEEDBACK:"):
            feedback = line.split("FEEDBACK:")[1].strip()
        elif line.startswith("MESSAGE:"):
            message = line.split("MESSAGE:")[1].strip()
    
    updates["messages"].append(AIMessage(content=message))
    updates["feedback"] = feedback
    
    logger.info("Returning Command from critique_node - updates: %s, goto: testcase", updates)
    return Command(
        update=updates,
        goto="testcase"
    )

# Assemble the StateGraph
workflow = StateGraph(AgentState)
workflow.add_node("testcase", testcase_node)
workflow.add_node("critique", critique_node)

workflow.set_entry_point("testcase")
app = workflow.compile()

# Run the workflow
initial_state = {"messages": [], "testcase": None, "feedback": None, "code_context": None}
logger.info("Starting workflow with initial state: %s", initial_state)
result = app.invoke(initial_state)
logger.info("Workflow completed with final state: %s", result)
print("Final state:", result)
