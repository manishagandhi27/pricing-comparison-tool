from langchain.document_loaders import DoclingLoader
from langchain.embeddings import HuggingFaceEmbeddings
from elasticsearch import Elasticsearch, helpers

# Initialize
es = Elasticsearch(["http://localhost:9200"])
index_name = "my_chunks_index"
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Bulk index (unchanged for now - debug this next)
loader = DoclingLoader(file_path="report.pdf")  # Adjust path
chunks = loader.load()
bulk_actions = []
for i, chunk in enumerate(chunks):
    try:
        meta = chunk.metadata if isinstance(chunk.metadata, dict) else {"source": str(chunk.metadata) if chunk.metadata else ""}
        embedding = embeddings.embed_query(chunk.page_content)
        if not isinstance(embedding, list) or len(embedding) != 384:
            raise ValueError(f"Invalid embedding for chunk {i}: {embedding}")
        action = {
            "_op_type": "index",
            "_index": index_name,
            "_id": str(i),
            "_source": {
                "content": chunk.page_content,
                "dense_embedding": embedding,
                "metadata": meta,
                "tags": ["AI", "tech"],
                "file_description": "AI trends report",
                "file_name": "unknown.pdf"
            }
        }
        bulk_actions.append(action)
    except Exception as e:
        print(f"Error preparing chunk {i}: {str(e)}")
        continue

success, failed = helpers.bulk(es, bulk_actions, chunk_size=1000, raise_on_error=False)
print(f"Successfully indexed {success} chunks in bulk!")
if failed:
    print(f"Failed to index {len(failed)} chunks:")
    for fail in failed:
        print(f"Failed doc ID: {fail.get('index', {}).get('_id', 'unknown')}, Error: {fail.get('index', {}).get('error', 'unknown')}")

# Fixed hybrid search without reranker
def hybrid_search(query, top_k=5):
    query_embedding = embeddings.embed_query(query)
    hybrid_query = {
        "size": top_k,
        "retriever": {
            "rrf": {
                "retrievers": [
                    {
                        "standard": {
                            "query": {
                                "match": {
                                    "content": {"query": query, "boost": 1.0}  # BM25 baseline
                                }
                            }
                        }
                    },
                    {
                        "standard": {
                            "query": {
                                "bool": {
                                    "should": [
                                        {"semantic": {"field": "semantic_content", "query": query}}
                                    ],
                                    "boost": 2.0  # Boost sparse via bool
                                }
                            }
                        }
                    },
                    {
                        "knn": {
                            "field": "dense_embedding",
                            "query_vector": query_embedding,
                            "k": top_k,
                            "num_candidates": 100,
                            "boost": 2.0  # Boost dense
                        }
                    }
                ],
                "rank_constant": 20,  # Sharper rank drop-off
                "window_size": 50  # Limit ranked docs
            }
        }
    }

    response = es.search(index=index_name, body=hybrid_query)
    results = []
    for hit in response["hits"]["hits"]:
        meta = hit["_source"].get("metadata", {})
        result = {
            "score": f"{hit['_score']:.4f}",
            "content": hit["_source"]["content"],
            "tags": hit["_source"].get("tags", []),
            "file_name": hit["_source"].get("file_name", "unknown"),
            "file_description": hit["_source"].get("file_description", ""),
            "page": meta.get("dl_items", {}).get("page", "N/A") if "dl_items" in meta else "N/A",
            "source": meta.get("source", "N/A")
        }
        results.append(result)

    print(f"\nHybrid Search Results for '{query}' (Top {top_k}):")
    for i, res in enumerate(results, 1):
        print(f"{i}. Score: {res['score']}")
        print(f"   Content: {res['content']}")
        print(f"   Tags: {', '.join(res['tags']) or 'None'}")
        print(f"   File: {res['file_name']}")
        print(f"   Description: {res['file_description']}")
        print(f"   Page: {res['page']}")
        print(f"   Source: {res['source']}")
        print("---")
    print(f"Total hits: {response['hits']['total']['value']}")

    return results

# Test
query = "AI and machine learning"
results = hybrid_search(query)
